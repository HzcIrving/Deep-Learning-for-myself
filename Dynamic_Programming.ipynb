{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic_Programming.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HzcIrving/Deep-Learning-for-myself/blob/master/Dynamic_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31A81arG_jW2",
        "colab_type": "text"
      },
      "source": [
        "## ---动态规划的三个问题---\n",
        "1. Policy Evaluation 策略估计问题;\n",
        "  - Bellman Expectation Equation \n",
        "2. Policy Iteration 策略迭代问题;\n",
        "  - Bellman ... & Greedy Policy Improvement\n",
        "  - 包括：\n",
        "    - Policy Evaluation\n",
        "    - Policy Improvement \n",
        "3. Value Iteration 值迭代问题;\n",
        "  - Bellman Optimally Equation \n",
        "4. 主要场景，David Silver大神在动态规划章节中的SMALL GRIDWORLD场景，可以去课件看下...\n",
        "5. 注意，动态规划与强化学习不同，主要解决的是Planning问题，即系统的环境MDP信息是完全可知的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMo_Yy8uBeqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "794304bd-e2a9-45ec-88f9-f381d9876c3d"
      },
      "source": [
        "# 装在google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9_3exRzCHDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 工作区\n",
        "import os \n",
        "os.chdir('/content/drive/My Drive/DEMO')\n",
        "WORKSPACE = os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmKnlAsnBkpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "98e8d7f0-933d-4638-c953-ddffe960a188"
      },
      "source": [
        "# Git 仓库\n",
        "!git clone https://github.com/dennybritz/reinforcement-learning.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'reinforcement-learning'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1274 (delta 0), reused 0 (delta 0), pack-reused 1270\u001b[K\n",
            "Receiving objects: 100% (1274/1274), 5.25 MiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (819/819), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDeXPwfB3n_",
        "colab_type": "text"
      },
      "source": [
        "### 1.Policy Evaluation问题\n",
        "在Policy_evaluation中：\n",
        "  - 在给定环境和环境动态的完整描述的情况下评估策略；\n",
        "  - policy:[S,A]matrix代表policy\n",
        "  - env:OpenAI环境；\n",
        "    - env.P代表代表环境的转移概率。\n",
        "    - env.P[s][a]是转移矩阵的列表(prob,next_state,reward,done)\n",
        "    - env.nS代表环境中state数量;\n",
        "    - env.nA代表环境中action数量(东、南、西、北)\n",
        "  - theta: value function改变小于theta，停止评估;\n",
        "  - discount_factor: Gamma \n",
        "  - return: Value Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SFFZIkCm6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e48577ab-ef43-4f8c-efcc-30d5891f8298"
      },
      "source": [
        "# 导入相关库\n",
        "from IPython.core.debugger import set_trace\n",
        "import numpy as np\n",
        "import pprint\n",
        "from reinforcementlearning.lib.envs.gridworld import GridworldEnv\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore') #忽略警告\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2) # 方便打印\n",
        "env = GridworldEnv() #Gridworld环境\n",
        "\n",
        "def policy_evaluation(policy,env,discount_factor=1.0,theta=0.00001):\n",
        "  # 初始化,all 0 \n",
        "  # 注意，策略: n,e,s,w都是.25\n",
        "  V = np.zeros(env.nS)\n",
        "  while True:\n",
        "    delta = 0 \n",
        "    # 对于每一个state,执行\"full backup\"\n",
        "    for s in range(env.nS): \n",
        "      v = 0\n",
        "      # 查看可能的后续动作（已知MDP的基础上才可以）\n",
        "      # action_prob -> policy π\n",
        "      for a,action_prob in enumerate(policy[s]):\n",
        "        # 对于每一个动作，查看可能产生的所有next_state\n",
        "        for prob,next_state,reward,done in env.P[s][a]:\n",
        "          # 计算期望Value\n",
        "          v += action_prob*prob*(reward+discount_factor*V[next_state])\n",
        "      \n",
        "      # 计算值函数改变了多少\n",
        "      delta = max(delta,np.abs(v-V[s]))\n",
        "      V[s] = v\n",
        "    # stop条件\n",
        "    if delta < theta:\n",
        "      break \n",
        "  return np.array(V)\n",
        "\n",
        "random_policy = np.ones([env.nS, env.nA]) / env.nA #均匀random policy\n",
        "v = policy_evaluation(random_policy, env)\n",
        "\n",
        "print(\"Reshaped Grid Value Function:\")\n",
        "print(v.reshape(env.shape))\n",
        "print(\"\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped Grid Value Function:\n",
            "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
            " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
            " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
            " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUa_h5SHvoR",
        "colab_type": "text"
      },
      "source": [
        "### 2.Policy Iteration问题\n",
        "- 先估计policy π的好坏(Policy Eval);\n",
        "- 在根据贪婪策略选择选择$v_π$最大的π;\n",
        "- Policy_Evaluation:过程同上面Policy_Evaluation;\n",
        "- Policy_improvement:反复评估和改进策略直到找到最佳策略为止;\n",
        "  - env:环境\n",
        "  - policy_eval_fn:PolicyEvaluationFunction，这是一个迭代过程;\n",
        "  - discount_factor\n",
        "  - return:\n",
        "    - 一个元组(policy,V)\n",
        "    - policy是最优policy,策略是最优策略，形状为[S，A]的矩阵，其中每个状态s包含动作的有效概率分布;\n",
        "    - V是最优策略的价值函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvNEZc9sIJuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "6e987dd2-eccc-4ab4-ce6e-7c12e5495720"
      },
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "import sys\n",
        "# if \"../\" not in sys.path:\n",
        "#   sys.path.append(\"../\") \n",
        "from reinforcementlearning.lib.envs.gridworld import GridworldEnv\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "env = GridworldEnv()\n",
        "\n",
        "print(\"环境维数4x4grid:\",env.shape)\n",
        "print(\"动作维数:\",env.nA)\n",
        "print(\"状态维数:\",env.nS)\n",
        "\n",
        "def policy_evaluation(policy,env,discount_factor=1.0,theta=0.00001):\n",
        "  # 初始化,all 0 \n",
        "  # 注意，策略: n,e,s,w都是.25\n",
        "  V = np.zeros(env.nS)\n",
        "  while True:\n",
        "    delta = 0 \n",
        "    # 对于每一个state,执行\"full backup\"\n",
        "    for s in range(env.nS): \n",
        "      v = 0\n",
        "      # 查看可能的后续动作（已知MDP的基础上才可以）\n",
        "      # action_prob -> policy π\n",
        "      for a,action_prob in enumerate(policy[s]):\n",
        "        # 对于每一个动作，查看可能产生的所有next_state\n",
        "        for prob,next_state,reward,done in env.P[s][a]:\n",
        "          # 计算期望Value\n",
        "          v += action_prob*prob*(reward+discount_factor*V[next_state])\n",
        "      \n",
        "      # 计算值函数改变了多少\n",
        "      delta = max(delta,np.abs(v-V[s]))\n",
        "      V[s] = v\n",
        "    # stop条件\n",
        "    if delta < theta:\n",
        "      break \n",
        "  return np.array(V)\n",
        "\n",
        "def policy_improvement(env,policy_eval_fn=policy_evaluation,discount_factor=1.0):\n",
        "  def one_step_lookahead(state,V):\n",
        "    \"\"\"\n",
        "    辅助函数可计算给定状态下所有动作的值。\n",
        "    Args:\n",
        "      state:要考虑的状态（int）\n",
        "      V:用作估计量的值，长度为env.nS的向量\n",
        "    Return:\n",
        "      一个长度为env.nA的向量，其中包含每个动作的期望值。\n",
        "    \"\"\"\n",
        "    A = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "      for prob,next_state,reward,done in env.P[state][a]:\n",
        "        A[a] += prob*(reward+discount_factor*V[next_state])\n",
        "    return A \n",
        "  \n",
        "  # 从一个随机策略开始\n",
        "  policy = np.ones([env.nS,env.nA])/env.nA # 0.25\n",
        "  while True:\n",
        "    # 评估当前策略\n",
        "    V = policy_eval_fn(policy,env,discount_factor)\n",
        "    policy_stable = True # 若要更改策略，设置为False\n",
        "    \n",
        "    # 对于每一个状态\n",
        "    for s in range(env.nS):\n",
        "      # 我们将在当前政策下采取最佳措施\n",
        "      chosen_a = np.argmax(policy[s])\n",
        "      # 通过one-step lookahead来找到最佳动作\n",
        "      action_values = one_step_lookahead(s,V)\n",
        "      best_a = np.argmax(action_values)\n",
        "\n",
        "      # 贪婪更新policy\n",
        "      if chosen_a != best_a:\n",
        "        policy_stable = False \n",
        "      \n",
        "      # 生成维数为nA的对角矩阵,选择第best_a行\n",
        "      policy[s] = np.eye(env.nA)[best_a]\n",
        "    \n",
        "    # 如果是stable的\n",
        "    if policy_stable:\n",
        "      return policy,V\n",
        "\n",
        "\n",
        "# policy, v = policy_improvement(env)\n",
        "# print(\"Policy Probability Distribution:\")\n",
        "# print(policy)\n",
        "# print(\"\")\n",
        "\n",
        "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
        "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
        "print(\"\")\n",
        "\n",
        "# print(\"Value Function:\")\n",
        "# print(v)\n",
        "# print(\"\")\n",
        "\n",
        "print(\"Reshaped Grid Value Function:\")\n",
        "print(v.reshape(env.shape))\n",
        "print(\"\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "环境维数: [4, 4]\n",
            "动作维数: 4\n",
            "状态维数: 16\n",
            "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
            "[[0 3 3 2]\n",
            " [0 0 0 2]\n",
            " [0 0 1 2]\n",
            " [0 1 1 0]]\n",
            "\n",
            "Reshaped Grid Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3eS0WxGLG-5",
        "colab_type": "text"
      },
      "source": [
        "### 3.Value Iteration问题"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbQGtRf9OrSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "a1b790f7-9f4e-4661-d65e-83d6df811d11"
      },
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "import sys\n",
        "# if \"../\" not in sys.path:\n",
        "#   sys.path.append(\"../\") \n",
        "from reinforcementlearning.lib.envs.gridworld import GridworldEnv\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "env = GridworldEnv()\n",
        "\n",
        "print(\"环境维数4x4grid:\",env.shape)\n",
        "print(\"动作维数:\",env.nA)\n",
        "print(\"状态维数:\",env.nS)\n",
        "\n",
        "def value_iteration(env,theta=0.0001,discount_factor=1.0):\n",
        "  def one_step_lookahead(state,V):\n",
        "    \"\"\"\n",
        "    辅助函数可计算给定状态下所有动作的值。\n",
        "    Args:\n",
        "      state:要考虑的状态（int）\n",
        "      V:用作估计量的值，长度为env.nS的向量\n",
        "    Return:\n",
        "      一个长度为env.nA的向量，其中包含每个动作的期望值。\n",
        "    \"\"\"\n",
        "    A = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "      for prob,next_state,reward,done in env.P[state][a]:\n",
        "        A[a] += prob*(reward+discount_factor*V[next_state])\n",
        "    return A \n",
        "\n",
        "  V = np.zeros(env.nS) # 初始化V值表\n",
        "  while True:\n",
        "    # 停止条件\n",
        "    delta = 0 \n",
        "    # 更新each state...\n",
        "    for s in range(env.nS):\n",
        "      # 一次one-step lookahead来找到最佳action\n",
        "      A = one_step_lookahead(s,V)\n",
        "      best_action_value = np.max(A) #找最大Value\n",
        "      # 计算delta\n",
        "      delta = max(delta, np.abs(best_action_value - V[s]))\n",
        "      V[s] = best_action_value\n",
        "    \n",
        "    if delta<theta:\n",
        "      break \n",
        "  \n",
        "  # 建立一个确定性策略(使用最优值函数)\n",
        "  policy = np.zeros([env.nS,env.nA]) # 16x4 \n",
        "  for s in range(env.nS):\n",
        "    # 找到best action \n",
        "    A = one_step_lookahead(s,V)\n",
        "    best_action = np.argmax(A)\n",
        "    # 总采用最优策略\n",
        "    policy[s,best_action] = 1.0\n",
        "  \n",
        "  return policy,V \n",
        "\n",
        "policy, v = value_iteration(env)\n",
        "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
        "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
        "print(\"\")\n",
        "print(\"Reshaped Grid Value Function:\")\n",
        "print(v.reshape(env.shape))\n",
        "print(\"\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "环境维数4x4grid: [4, 4]\n",
            "动作维数: 4\n",
            "状态维数: 16\n",
            "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
            "[[0 3 3 2]\n",
            " [0 0 0 2]\n",
            " [0 0 1 2]\n",
            " [0 1 1 0]]\n",
            "\n",
            "Reshaped Grid Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOeUrSfBTBX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}