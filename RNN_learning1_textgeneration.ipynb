{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_learning1_textgeneration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HzcIrving/Deep-Learning-for-myself/blob/master/RNN_learning1_textgeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYejVwxhq9sE",
        "colab_type": "text"
      },
      "source": [
        "#  使用RNN来生成文本\n",
        "- **工具** -- 使用character-based RNN.  \n",
        "- **数据集** -- 使用Shakersperare's writing\n",
        "  - 给定一段莎士比亚风格的字符串，训练模型，反复调用模型来自动生成这种风格的诗词。\n",
        "  - 给GPU使能来获得更快的执行速度：\n",
        "    -  In Colab: Runtime > Change runtime type > Hardware acclerator > GPU.\n",
        "- 这个教程保护可执行代码实现（通过调用keras [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). 下面这个是输出的样本，当教程训练30个epoch，然后以“Q”为开始输出的话剧风格：\n",
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "* 这个模型是基于字符串的，当训练开始时，这个模型不知道如何拼写英语单词。\n",
        "* 输出的结构类似于文本的播放块，通常以说话者名称开头，使用与数据集类似的所有大写字母。\n",
        "*如下图所示，该模型针对小批文本(每批100个字符)进行训练，仍然能够生成具有连贯结构的较长的文本序列"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6wACZIYtzYw",
        "colab_type": "text"
      },
      "source": [
        "## 1. 配置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GCeO3tuJKs",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 导入库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBOupEtuM85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf \n",
        "tf.enable_eager_execution()\n",
        "\"\"\"\n",
        "TensorFlow 的 Eager Execution 是一种命令式编程环境，\n",
        "可立即评估操作，无需构建图：操作会返回具体的值，而\n",
        "不是构建以后再运行的计算图。这样能让您轻松地开始使\n",
        "用 TensorFlow 和调试模型，并且还减少了样板代码。\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np \n",
        "import os \n",
        "import time "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNCX2_2suc4J",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 下载莎士比亚数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_4idwKNu4R_",
        "colab_type": "code",
        "outputId": "dc9d3de7-e351-4512-90a7-0250142225b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "  'shakespeare.txt',\n",
        "  'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfz21c-YvGbR",
        "colab_type": "text"
      },
      "source": [
        "## 1.3  阅读数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGN8CV2evVQ3",
        "colab_type": "code",
        "outputId": "9af55656-b068-44be-92b5-93b294631d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "# 文本长度，表示是文本中包含字符串的总数量\n",
        "print('文本长度:{} 字符'.format(len(text)))\n",
        "\n",
        "# 看一下头250个字符\n",
        "print(text[:250])\n",
        "\n",
        "# 文件中的独特字符\n",
        "vocab = sorted(set(text))\n",
        "print('{} 特殊字符'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "文本长度:1115394 字符\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 特殊字符\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx_QF-HivbHP",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 处理文本\n",
        "### 1.4.1 对文本进行向量化\n",
        "在训练之前，我们需要将字符串映射为字符表示。我们需要生成两个查字表，一个将字符映射到数字，另一个将数字映射到字符。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5vc79AwgFh",
        "colab_type": "code",
        "outputId": "da817505-8360-4c42-fe22-afdc2ec9d18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# 将特殊字符映射到索引\n",
        "char2idx = {u: i for i,u in enumerate(vocab)} # i:index,u:char\n",
        "print(char2idx)\n",
        "idx2char = np.array(vocab)\n",
        "print(idx2char)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(text_as_int) # 输出text中每个字符对应的index\n",
        "print(text_as_int.shape)\n",
        "print(text_as_int[1:10])\n",
        "print(text[1:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "[18 47 56 ... 45  8  0]\n",
            "(1115394,)\n",
            "[47 56 57 58  1 15 47 58 47]\n",
            "irst Citi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKWL3yNTxW4m",
        "colab_type": "text"
      },
      "source": [
        "本教程：如何使用基于字符的RNN生成文本：\n",
        "- Andrej Karpathy 在 The Unreasonable Effectiveness of Recurrent Neural Networks 一文中提供的莎士比亚作品数据集；\n",
        "- 虽然有些句子合乎语法规则，但大多数句子都没有意义。该模型尚未学习单词的含义。\n",
        "  -  该模型是基于字符的模型。在训练之初，该模型都不知道如何拼写英语单词，甚至不知道单词是一种文本单位。\n",
        "  - 输出的文本结构仿照了剧本的结构：文本块通常以讲话者的名字开头，并且像数据集中一样，这些名字全部采用大写字母。\n",
        "  - 如下文所示，尽管该模型只使用小批次的文本（每批文本包含 100 个字符）训练而成，但它仍然能够生成具有连贯结构的更长文本序列。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl8wkGoZrKMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6e8931ba-fd97-44ac-d466-dd5a2464bbed"
      },
      "source": [
        "# 库\n",
        "import tensorflow as tf \n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np \n",
        "import os \n",
        "import time \n",
        "\n",
        "# 下载数据集\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3OsHGqYrUXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 读取数据\n",
        "# 看一些文本内容\n",
        "text = open(path_to_file).read()\n",
        "# text的长度，是里面所有字符数\n",
        "print('text字符长度数: {}'.format(len(text)))\n",
        "\n",
        "# 看一下头1000个字符\n",
        "print(text[:1000])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGm84vkar6pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 处理文本\n",
        "# 1.向量化\n",
        "# 将字符串映射到数字表示值。\n",
        "\"\"\"创建两个对照表：\n",
        "一个用于将字符映射到数字，\n",
        "另一个用于将数字映射到字符。\"\"\"\n",
        "\n",
        "# 查看特殊字符'\n",
        "vocab = sorted(set(text))\n",
        "print('{}种字符'.format(len(vocab)))\n",
        "\n",
        "# 创建一个特殊字符到index的映射\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "# print(char2idx)\n",
        "# 每一个字符都有一个对应的整数表示值，从0到len(vocab)的索引映射\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]) \n",
        "# print(text_as_int)\n",
        "\n",
        "for char,_ in zip(char2idx,range(20)):\n",
        "  print('{:6s}-->{:4d}'.format(repr(char),char2idx[char]))\n",
        "  \n",
        "# 显示文本的前13个字符如何映射到整数\n",
        "print('{} ---- characters mapped to int ---->{}'.format(text[:13],text_as_int[:13]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b5RDebYO-4M",
        "colab_type": "text"
      },
      "source": [
        "#预测任务\n",
        "根据给定的字符或字符序列预测下一个字符最有可能是什么？这是我们要训练模型去执行的任务。模型的输入将是字符序列，而我们要训练模型去预测输出，即每一个时间步的下一个字符。<br>\n",
        "由于 RNN 会依赖之前看到的元素来维持内部状态，那么根据目前为止已计算过的所有字符，下一个字符是什么？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOePIcTTRXID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 创建训练样本和目标\n",
        "# 将文本化为训练样本和训练目标。\n",
        "\"\"\"每个训练样本都包含从文本中选取的 seq_length 个字符。\n",
        "相应的目标也包含相同长度的文本，但是将所选的字符序列\n",
        "向右顺移一个字符。例如，假设 seq_length 为 4，我们的\n",
        "文本为“Hello”，则可以将“Hell”创建为训练样本，将\n",
        "“ello”创建为目标。\"\"\"\n",
        "\n",
        "# 将文本拆分为文本块，每个块长度为seq_length+1\n",
        "# 我们希望单个字符输入的最大长度句\n",
        "seq_length = 100\n",
        "\n",
        "# 建立训练样本以及训练目标\n",
        "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1,\\\n",
        "                                            drop_remainder = True)\n",
        "\n",
        "for item in chunks.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()]))) # repr\n",
        "\n",
        "# 利用此文本块创建输入文本和目标文本\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text,target_text \n",
        "\n",
        "# 将文本块映射到上面的函数\n",
        "dataset = chunks.map(split_input_target)\n",
        "         \n",
        "# 输出第一个样本前10个值\n",
        "print('\\n'*2)\n",
        "for input_example,target_example in dataset.take(1):\n",
        "  print('训练样本:',repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('训练目标:',repr(''.join(idx2char[target_example.numpy()])))\n",
        "print('\\n'*2)\n",
        "\n",
        "# 这些向量每个索引均作为一个时间步来处理\n",
        "# 时间步0，输入：映射到18的字符，并尝试预测映射到47的字符；\n",
        "# 时间步1，相同操作，除了当前字符外，还要考虑上一步信息\n",
        "for i,(input_idx,target_idx) in enumerate(zip(input_example[:5],target_example[:5])):\n",
        "  print(\"时间步：{:4d}\".format(i))\n",
        "  print(\"输入：{}({:s})\".format(input_idx,repr(idx2char[input_idx])))\n",
        "  print(\"期望输出：{}({:s})\".format(target_idx,repr(idx2char[target_idx])))\n",
        "print('\\n'*2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k1x_v41X--p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 使用tf.data创建批次文本并重排这些批次\n",
        "\"\"\"使用tf.data将文本分块，在将这些数据馈送到模型中之前，需要对数据进行\n",
        "重排，并将其打包成批\"\"\"\n",
        "\n",
        "# Batch size \n",
        "BATCH_SIZE  = 64\n",
        "\n",
        "# BUFFERSIZE来shuffle数据集\n",
        "# TF数据被设计用来处理可能无限的序列,所以它不会试图在内存中打乱整个序列\n",
        "# 实际上，其维护一个缓冲区，在其中对元素进行shuffle\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
        "\n",
        "# 模型\n",
        "  # 嵌入层：一个可训练的对照表，它会将每个字符的数字映射到具有 embedding_dim 个维度的高维度向量；\n",
        "  # GRU层：一种层大小等于单位数的 RNN。（在此示例中，您也可以使用 LSTM 层。）\n",
        "  # 密集层：带有vocab_size个单元\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,units):\n",
        "    super(Model,self).__init__()\n",
        "    self.units = units \n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    \n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units,\n",
        "                                        return_sequences=True,\n",
        "                                        recurrent_initializer='glorot_uniform',\n",
        "                                        stateful=True)\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   recurrent_initializer='glorot_unifrom',\n",
        "                                   stateful=True)\n",
        "      \n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)      \n",
        "  \n",
        "  def call(self,x):\n",
        "    embedding = self.embedding(x)\n",
        "    \n",
        "    # 每个时间步输出\n",
        "    # 输出shape = (batch_size,seq_length,hidden_size)\n",
        "    output = self.gru(embedding)\n",
        "    \n",
        "    # dense layer会输出每个时间步（seq_length）的预测值\n",
        "    # 在denselayer之后的输出shape = (seq_length*batch_size,vocab_size)\n",
        "    prediction = self.fc(output)\n",
        "    \n",
        "    # 在训练过程中，状态将被用于通过模型的每一步\n",
        "    return prediction \n",
        "\n",
        "# 实例化模型、优化器与Loss function\n",
        "# vocabulary字符长度\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# embedding 维度\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN单元数量\n",
        "units = 1024\n",
        "\n",
        "model = Model(vocab_size,embedding_dim,units)\n",
        "\n",
        "# 优化器 adam \n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# 使用系数交叉熵,不需要建立one-hot\n",
        "def loss_function(real,preds):\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels=real,logits=preds)\n",
        "\n",
        "model.build(tf.Tensorshape([BATCH_SIZE,seq_length]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OLz1qWFlRJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b623733e-cc28-4744-c70a-42e04b2c300c"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "print('\\n'*2)\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "print('\\n'*2)\n",
        "\n",
        "# 对于输入字符串的期望最大序列长度\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 建立训练样本与训练目标\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])\n",
        "print('\\n'*2)\n",
        "  \n",
        "# 批处理方法允许我们轻松地将这些单个字符转换为所需大小的序列。\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n",
        "print('\\n'*2)  \n",
        "  \n",
        "# 定义划分函数，并映射到数据库\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "print('\\n'*2)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
        "print('\\n'*2)\n",
        "\n",
        "# Batch Size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
        "# dataset\n",
        "\n",
        "# # 构建模型\n",
        "# 嵌入层：一个可训练的对照表，它会将每个字符的数字映射到具有 embedding_dim 个维度的高维度向量；\n",
        "# GRU 层：一种层大小等于单位数的 RNN。（在此示例中，您也可以使用 LSTM 层。）\n",
        "# 密集层：带有 vocab_size 个单元。\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools \n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid'\n",
        "  )\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "# 检查输出维度\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "print('\\n'*2)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 训练模型\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(\n",
        "  optimizer = tf.train.AdamOptimizer(),\n",
        "   loss = loss\n",
        ")\n",
        "\n",
        "# -------------配置检查点--------------------------------------\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "EPOCHS = 3\n",
        "history = model.fit(\n",
        "      dataset.repeat(),\n",
        "      epochs = EPOCHS,\n",
        "      steps_per_epoch = steps_per_epoch,\n",
        "      callbacks=[checkpoint_callback]\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "\n",
            "\n",
            "\n",
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "\n",
            "\n",
            "\n",
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "\n",
            "\n",
            "\n",
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n",
            "\n",
            "\n",
            "\n",
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n",
            "\n",
            "\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_4 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1752224\n",
            "Epoch 1/3\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 2.6493\n",
            "Epoch 2/3\n",
            "174/174 [==============================] - 24s 137ms/step - loss: 1.9282\n",
            "Epoch 3/3\n",
            "174/174 [==============================] - 24s 137ms/step - loss: 1.6709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eKT0IrlyqG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "outputId": "d21ce23b-ef73-4b72-a8a8-81122c0e6024"
      },
      "source": [
        "# 产生文本\n",
        "# 重新读取最近的检查点\n",
        "# 模型仅接受固定大小的批次数据。\n",
        "# 要使用相同的权重和不同的模型，我们需要重建模型，并从检查点恢复权重。\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "\"\"\"\n",
        "由于RNN状态从一个时间步传递到另一个时间步的方式，\n",
        "模型只接受构建后的固定批大小。\n",
        "\n",
        "要运行具有不同批大小的模型，\n",
        "我们需要重新构建模型并从检查点恢复权重。\n",
        "\"\"\"\n",
        "model = build_model(vocab_size,embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "model.summary()\n",
        "\n",
        "def generate_text(model,start_string):\n",
        "  # 产生的字符数\n",
        "  num_generate = 1000\n",
        "  \n",
        "  # 将开始字符串转换为数字(向量化)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "  \n",
        "  # 空列表，用来存储结果\n",
        "  text_generated = []\n",
        "  \n",
        "  # temperature越低>>预测越准\n",
        "  # temperature越高>>结果越出人意料\n",
        "  temperature = 1.0\n",
        "  \n",
        "  # 这里batch_size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # 移除batch维度\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "    # 然后，使用多项分布计算预测字符的索引\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.multinomial(predictions,num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    # 将这个作为输入\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model,start_string =u\"ROMEO:\"))\n",
        "# 提升表现：\n",
        "  # 1.EPOCHS提高\n",
        "  # 2.不同的start_string、temperature\n",
        "  # 3.改变网络结构"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_10 (CuDNNGRU)      (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "ROMEO:\n",
            "Canminar, for Rome, I have; strick'd, but his not your were,\n",
            "To gentle not, I have king deap of her,\n",
            "As would to this neviur; neeling for thy same, four och his bears, and made us will yourselve's dispartion.\n",
            "First, of lords in throunct is cannot wasces be with sus\n",
            "The imingl'd velaunter:\n",
            "For beion! we speak upain to you con of any leftlemen?\n",
            "I be a fullent and brother breathing there.\n",
            "\n",
            "GLOUCESTER:\n",
            "To goter prayer'd the with the yourse that of lloft\n",
            "Now day not aster gones: and I follow men you: This will; thou desermer!\n",
            "\n",
            "queec,\n",
            "Hut praysicelf, I cannot can be note!\n",
            "\n",
            "WARWICK:\n",
            "Honce! Yould his kind?\n",
            "\n",
            "KING EDWARD IV: bright my love,\n",
            "That have make this pause up A hand; and for theirasue tuln thee;\n",
            "I take thesey and thus I take upon.\n",
            "\n",
            "KATHANINA:\n",
            "Petsure with her each, I had horly to him;\n",
            "Me bettuce, usching not, gome them moruboue claugit of disgages\n",
            "And prove you fear, hil! come all perrawed they;\n",
            "Rome? Come, be reading thou but a floping it-becking for! Yea, sin, me,\n",
            "I'll stay ty thoug\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwuMhCOQTUI5",
        "colab_type": "text"
      },
      "source": [
        "###在此示例中，我们使用采用 GradientTape 的自定义训练循环。要详细了解此方法，请参阅 Eager Execution 指南。\n",
        "\n",
        "- 首先，用零和形状（批次大小，RNN 单元数）初始化模型的隐藏状态。为此，我们将调用在创建模型时定义的函数。\n",
        "\n",
        "- 然后，逐批对数据集进行迭代，并计算与该输入关联的预测和隐藏状态。\n",
        "\n",
        "- 在训练过程中，发生了许多有趣的现象：\n",
        "\n",
        "  - 模型获得隐藏状态（初始化为 0，我们称之为 H0）和第一批输入文本（我们称之为 I0）。\n",
        "  - 然后，模型返回预测值 P1 和 H1。\n",
        "  - 对于下一批输入，模型收到 I1 和 H1。\n",
        "  - 现在，有趣的是我们将 H1 随 I1 一起传递给模型，模型正是通过这种方式进行学习。从各个批次中学习到的上下文将包含到隐藏状态中。\n",
        "  - 重复上述操作，直到数据集中的数据全部用尽。然后开始一个新的周期，并重复此过程。\n",
        "  - 计算预测值后，使用上面定义的损失函数计算损失。然后，计算相对于模型变量的损失梯度。\n",
        "\n",
        "- 最后，使用 apply_gradients 函数在优化器的帮助下朝着训练的方向迈进一步。\n",
        "![训练过程](https://github.com/mari-linhares/docs/blob/patch-1/site/en/tutorials/sequences/images/text_generation_training.png?raw=true)\n",
        "\n",
        "### 使用模型生成文本\n",
        "使用我们训练的模型生成文本<br>\n",
        "下面的代码块可生成文本：\n",
        "\n",
        "- 首先选择一个起始字符串，初始化隐藏状态，并设置要生成的字符数。\n",
        "\n",
        "- 使用起始字符串和隐藏状态获取预测值。\n",
        "\n",
        "- 然后，使用多项分布计算预测字符的索引 - 将此预测字符用作模型的下一个输入。\n",
        "\n",
        "- 模型返回的隐藏状态被馈送回模型中，使模型现在拥有更多上下文，而不是仅有一个单词。在模型预测下一个单词之后，经过修改的隐藏状态再次被馈送回模型中，模型从先前预测的单词获取更多上下文，从而通过这种方式进行学习。\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png)\n",
        "\n",
        "-----------------------------------------------------------------------------\n",
        "- repr() --> 返回对象的规范字符串表示形式。\n",
        "- list[:-1] --> 保存列表中除最后一个以外的所有值\n",
        "- list[1:] --> 保存列表中除第一个以外的所有值\n",
        "- batch(batch_size,drop_remainder=False)\n",
        "  -  drop_remainder:表示在少于batch_size元素的情况下是否应删除最后一批\n",
        "  -  [数据对象Dataset的详解](https://www.imooc.com/article/68648)\n",
        "- from_tensor_slices（tensors）\n",
        "  - 用于创建dataset，其元素是给定张量的切片的元素。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9--b5oRMTXpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab726ff6-ca05-47ef-8607-126f7b83681b"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTfakmoYT6zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}